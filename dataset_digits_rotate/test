import tensorflow as tf
import keras as keras
import keras.layers as layers
from keras.models import Sequential
import sklearn.datasets as datasets
from keras.layers import Dense
from sklearn.datasets import load_iris
from sklearn import tree
import numpy as np
import matplotlib.pyplot as plt
import time
from matplotlib.colors import ListedColormap
from keras import backend as K
#import draw_model
import data
import robustness_calculator
import draw_robustness
import os
import numpy as np
import keras
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.models import load_model
from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten
from keras.optimizers import Adam
import draw_high
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"]='1' # 这是默认的显示等级，显示所有信息
os.environ["TF_CPP_MIN_LOG_LEVEL"]='2' # 只显示 warning 和 Error
os.environ["TF_CPP_MIN_LOG_LEVEL"]='3'

#os.environ["CUDA_VISIBLE_DEVICES"] = "1"

#gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)

#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))


count_correct_close_list = []
count_wrong_list = []
rand_list = []
model_robustness_list = []
iter_no_list = []
test_accuracy_list = []
model_orig_robustness_list = []
time_total = 0
time_orig_total = 0


data_orig = data.data_orig
count_feature = data.count_feature
count_data = data.count_data
count_label = 2
count_train = int(0.9*count_data)
count_test = int(0.1*count_data)

x_train_orig = data_orig[:-count_test, 0:count_feature]
x_train = x_train_orig
x_test = data_orig[-count_test:, 0:count_feature]
y_train_orig = data_orig[:-count_test, count_feature]
y_train = y_train_orig
y_test_orig = data_orig[-count_test:, count_feature]
y_test = keras.utils.to_categorical(y_test_orig, num_classes=count_label)
#data_train_orig = np.hstack((x_train_orig, y_train_orig.reshape(count_train, 1), np.arange(1, count_train+1, 1).reshape(count_train, 1), data.radius*np.ones((count_train, 1)), 10*np.ones((count_train, 1))))
data_train_orig = data_orig[0: count_train, :]
y_orig = keras.utils.to_categorical(y_train_orig, num_classes=count_label)
count_column = data_train_orig[0, :].size

model = Sequential()
model.add(Conv2D(input_shape=(8, 8, 1), kernel_size=(5, 5), filters=20, activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))
model.add(Conv2D(kernel_size=(5, 5), filters=50, activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])


model_orig = Sequential()
model_orig.add(Conv2D(input_shape=(8, 8, 1), kernel_size=(5, 5), filters=20, activation='relu'))
model_orig.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))
model_orig.add(Conv2D(kernel_size=(5, 5), filters=50, activation='relu', padding='same'))
model_orig.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'))
model_orig.add(Flatten())
model_orig.add(Dense(500, activation='relu'))
model_orig.add(Dense(2, activation='softmax'))
model_orig.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model.load_weights('my_model_weights.h5')
model_orig.load_weights('my_model_weights.h5')

for iter_no in range(20):

    pred_orig = model.predict(x_train_orig.reshape(-1, 8, 8, 1))
    pred_orig = np.argmax(pred_orig, axis=1).reshape(count_train, 1)
    correct = np.equal(y_train_orig.reshape(count_train, 1), pred_orig)+0
    wrong = np.ones((count_train, 1))-correct
    index = np.arange(1, count_train+1).reshape(count_train, 1).flatten()

    no_wrong = wrong.flatten()*index
    no_correct = correct.flatten()*index

    class_wrong = np.empty([0, count_column])
    class_correct = np.empty([0, count_column])


    new_correct_close = np.empty((0, count_feature+1))
    new_correct_far = np.empty((0, count_feature+1))
    new_wrong_close = np.empty((0, count_feature+1))
    new_wrong_far = np.empty((0, count_feature+1))

    count_correct_close = 0
    count_correct_far = 0
    count_wrong_close = 0
    count_wrong_far = 0

    time_start = time.time()
    for value in no_wrong:
        if value != 0:
            point = data_train_orig[int(value)-1, :]
            _, pred_points, _ = robustness_calculator.point_generator(model, point, point[-2], np.array(4))
            pred_point = model.predict(point[0:count_feature].reshape(-1, 8, 8, 1))
            pred_point = np.argmax(pred_point).astype(np.int)
            if (pred_points == pred_point).all():
                data_train_orig[int(value)-1, :][-2] = data_train_orig[int(value)-1, :][-2]*1.1
                data_train_orig[int(value) - 1, :][-1] = data_train_orig[int(value)-1, :][-1]+3
                point = data_train_orig[int(value) - 1, :]
                bag_wrong_far = robustness_calculator.bag_generator(model, point, point[-2], point[-1])
                new_wrong_far = np.append(new_wrong_far, bag_wrong_far, axis=0)
                count_wrong_far += 1
            else:
                data_train_orig[int(value) - 1, :][-2] = data_train_orig[int(value) - 1, :][-2] * 0.9
                data_train_orig[int(value) - 1, :][-1] = data_train_orig[int(value) - 1, :][-1]+2
                point = data_train_orig[int(value) - 1, :]
                bag_wrong_close = robustness_calculator.bag_generator(model, point, point[-2], point[-1])
                new_wrong_close = np.append(new_wrong_close, bag_wrong_close, axis=0)
                count_wrong_close += 1
            #class_wrong = np.append(class_wrong, point.reshape(1, count_column), axis=0)


    for value in no_correct:
        if value != 0:
            point = data_train_orig[int(value) - 1, :]
            #if point[5] <= 1:
                #continue
            _, pred_points, _ = robustness_calculator.point_generator(model, point, point[-2], np.array(4))
            pred_point = model.predict(point[0:count_feature].reshape(-1, 8, 8, 1))
            pred_point = np.argmax(pred_point).astype(np.int)
            if (pred_points == pred_point).all():
                if data_train_orig[int(value) - 1, :][-1] > 2:
                    data_train_orig[int(value) - 1, :][-1] = data_train_orig[int(value) - 1, :][-1] - 1
                    data_train_orig[int(value) - 1, :][-2] = data_train_orig[int(value) - 1, :][-2] * 1.1
                    point = data_train_orig[int(value) - 1, :]
                    bag_correct_far = robustness_calculator.bag_generator(model, point, point[-2], point[-1])
                    new_correct_far = np.append(new_correct_far, bag_correct_far, axis=0)
                count_correct_far += 1
            else:
                data_train_orig[int(value) - 1, :][-2] = data_train_orig[int(value) - 1, :][-2] * 0.9
                data_train_orig[int(value) - 1, :][-1] = data_train_orig[int(value) - 1, :][-1] + 1
                point = data_train_orig[int(value) - 1, :]
                bag_correct_close = robustness_calculator.bag_generator(model, point, point[-2], point[-1])
                new_correct_close = np.append(new_correct_close, bag_correct_close, axis=0)
                count_correct_close += 1
            #class_correct = np.append(class_correct, point.reshape(1, count_column), axis=0)

    data_attack = np.vstack((new_wrong_close, new_wrong_far, new_correct_close, new_correct_far))
    x_attack = data_attack[:, 0:count_feature]
    y_attack = data_attack[:, count_feature]

    x_train = np.vstack([x_train_orig, x_attack])
    y_train = np.append(y_train_orig, y_attack)


    np.random.seed(iter_no+100)
    np.random.shuffle(x_train)
    np.random.seed(iter_no + 100)
    np.random.shuffle(y_train)

    y_train = keras.utils.to_categorical(y_train, num_classes=count_label)
    x_train = x_train.reshape(-1, 8, 8, 1)
    x_test = x_test.reshape(-1, 8, 8, 1)
    model.fit(x_train, y_train, epochs=2, batch_size=16, validation_data=(x_test, y_test))
    time_end = time.time()
    time1 = time_end - time_start
    time_total = time_total + time1
    pred_test = model.predict(x_train_orig.reshape(-1, 28, 28, 1))
    pred_test = np.argmax(pred_test, axis=1).reshape(count_train, 1)
    correct_test = np.equal(y_train_orig.reshape(count_train, 1), pred_test) + 0
    test_accuracy = correct_test.sum().astype(np.int32) / count_train

    print('accu' + str(test_accuracy))

    iter_no_list.append(iter_no)
    time_start = time.time()
    model_orig.fit(x_train_orig.reshape(-1, 8, 8, 1), y_orig, epochs=2, batch_size=16,
                   validation_data=(x_test, y_test))
    test_accuracy_list.append(test_accuracy)
    time_end = time.time()
    time_orig = time_end - time_start
    time_orig_total = time_orig + time_orig_total
    # y_r = draw_high.draw_high(model, iter_no, 'model')
    # model_robustness = draw_high.robustness_calculator(y_r)
    model_robustness = robustness_calculator.robustness_calculator(model, data_orig)
    print(str(iter_no) + 'model:' + str(model_robustness))
    model_robustness_list.append(model_robustness)
    # y_r_orig = draw_high.draw_high(model_orig, iter_no, 'model_orig')
    # model_orig_robustness = draw_high.robustness_calculator(y_r_orig)
    model_orig_robustness = robustness_calculator.robustness_calculator(model_orig, data_orig)
    print(str(iter_no) + 'model_orig:' + str(model_orig_robustness))
    model_orig_robustness_list.append(model_orig_robustness)
    print(str(iter_no) + 'time:' + str(time_total))
    print(str(iter_no) + 'time_orig:' + str(time_orig_total))
    print(iter_no)

figure = draw_robustness.draw_robustness_accuracy(iter_no_list, model_orig_robustness_list, model_robustness_list)
plt.savefig('robustness')
