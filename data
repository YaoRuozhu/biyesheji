import gzip
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.utils import to_categorical

IMAGE_SIZE = 28
NUM_CHANNELS = 1
PIXEL_DEPTH = 255
NUM_LABELS = 10
VALIDATION_SIZE = 5000  # Size of the validation set.
SEED = 66478  # Set to None for random seed.
BATCH_SIZE = 64
NUM_EPOCHS = 10
EVAL_BATCH_SIZE = 64
EVAL_FREQUENCY = 100  # Number of steps between evaluations.
filename = "D:/tf_cnn_mnist/mnist"

radius = 25
bagsize = 10


def extract_data(filename, num_images):
  with gzip.open(filename) as bytestream:
    bytestream.read(16)
    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)
    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)
    data = data / PIXEL_DEPTH
    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)
    return data


def extract_labels(filename, num_images):
  with gzip.open(filename) as bytestream:
    bytestream.read(8)
    buf = bytestream.read(1 * num_images)
    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)
  return labels


train_data_filename = 'D:/ruozhu/robust/more_robust/dataset4/mnist/train-images-idx3-ubyte.gz'  #训练集图像的文件名
train_labels_filename = 'D:/ruozhu/robust/more_robust/dataset4/mnist/train-labels-idx1-ubyte.gz'  #训练集label的文件名
test_data_filename = 'D:/ruozhu/robust/more_robust/dataset4/mnist/t10k-images-idx3-ubyte.gz'    #测试集图像的文件名
test_labels_filename = 'D:/ruozhu/robust/more_robust/dataset4/mnist/t10k-labels-idx1-ubyte.gz'    #测试集label的文件名

#train_data_filename = '/home/ifpp/ruozhu/dataset4/mnist/train-images-idx3-ubyte.gz'  #训练集图像的文件名
#train_labels_filename = '/home/ifpp/ruozhu/dataset4/mnist/train-labels-idx1-ubyte.gz'  #训练集label的文件名
#test_data_filename = '/home/ifpp/ruozhu/dataset4/mnist/t10k-images-idx3-ubyte.gz'    #测试集图像的文件名
# test_labels_filename = '/home/ifpp/ruozhu/dataset4/mnist/t10k-labels-idx1-ubyte.gz'


train_data = extract_data(train_data_filename, 50000)
train_labels = extract_labels(train_labels_filename, 50000).reshape(50000, 1)
test_data = extract_data(test_data_filename, 10000)
test_labels = extract_labels(test_labels_filename, 10000).reshape(10000, 1)

train_data_flat = train_data.reshape((50000, 28 * 28))
#train_data_flat = train_data.reshape((50000, ))
train_data = train_data_flat.astype('float32') / 255
test_data_flat = test_data.reshape((10000, 28 * 28))
test_data = test_data_flat.astype('float32') / 255

train_data = train_data.reshape(-1, 28, 28, 1)  # normalize
test_data = test_data.reshape(-1, 28, 28, 1)      # normalize

count_feature = train_data[0, :].size
count_label = 10
count_data = 10000

#x_orig = np.vstack((train_data_flat, test_data_flat))
x_orig = train_data_flat
x_orig = x_orig[0: count_data, :]
#y_orig = np.vstack((train_labels, test_labels))
y_orig = train_labels
y_orig = y_orig[0: count_data, :]
data_orig = np.hstack((x_orig, y_orig, np.arange(1, count_data+1, 1).reshape(count_data, 1), radius*np.ones([count_data, 1]), bagsize * np.ones([count_data, 1])))

data = [i for i in range(count_label)]
std = [i for i in range(count_label)]
for j in range(count_label):
    data[j] = np.empty((0, count_feature+4))
for i in range(0, count_data):
    for j in range(count_label):
        if data_orig[i, count_feature] == j:
            data[j] = np.vstack((data[j], data_orig[i, :]))

data_orig = np.vstack([data[0], data[1]])
count_data = data_orig[:, 0].size
a = data_orig[:, 0].reshape(count_data, 1)
b = data_orig[:, 1].reshape(count_data, 1)
x_data = np.hstack((a, b))
c = data_orig[:, 2:]
data_orig = np.hstack((x_data, c))
#test = data_orig[900, :]
#test_image = test[0: count_feature].reshape([28, 28])
#test_label = test[count_feature]
#plt.figure()
#plt.imshow(test_image, cmap=plt.cm.gray)
#plt.show()
#data_show = np.empty((0, count_feature))
#label_show = np.empty((0, 1))
#a = np.random.randint(0, count_data, (count_data, 1))
#for i in a:
  #point = data_orig[i, 0: count_feature]
  #label = data_orig[i, count_feature]
  #data_show = np.vstack((data_show, point))
  #label_show = np.vstack((label_show, label))
